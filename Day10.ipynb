{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a311410-ba44-4954-9717-4b65a02e8255",
   "metadata": {},
   "source": [
    "1. Handling Missing Data in Titanic Dataset\n",
    "   - Task:Identify and handle missing values in the Titanic dataset. Experiment with different strategies such as mean/median imputation, mode imputation, and dropping rows/columns.\n",
    "   - Dataset: Titanic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92437749-445d-4d26-b17e-9a118cb5325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "Missing values in each column:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "Dataset after handling missing values:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Embarked  \n",
      "0      0         A/5 21171   7.2500        S  \n",
      "1      0          PC 17599  71.2833        C  \n",
      "2      0  STON/O2. 3101282   7.9250        S  \n",
      "3      0            113803  53.1000        S  \n",
      "4      0            373450   8.0500        S  \n",
      "Missing values in each column after handling:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "print(\"Initial dataset:\")\n",
    "print(titanic.head())\n",
    "\n",
    "print(\"Missing values in each column:\")\n",
    "print(titanic.isnull().sum())\n",
    "\n",
    "titanic['Age'].fillna(titanic['Age'].mean(), inplace=True)\n",
    "titanic['Fare'].fillna(titanic['Fare'].median(), inplace=True)\n",
    "\n",
    "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "titanic.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "print(\"Dataset after handling missing values:\")\n",
    "print(titanic.head())\n",
    "\n",
    "print(\"Missing values in each column after handling:\")\n",
    "print(titanic.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ab7df-8445-4086-9fb4-d21d391fae7e",
   "metadata": {},
   "source": [
    "2. Encoding Categorical Variables in a Car Evaluation Dataset\n",
    "   - Task: Encode categorical variables in the Car Evaluation dataset using one-hot encoding and label encoding. Compare the results.\n",
    "   - Dataset: Car Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cac02-f34d-40ed-a379-72e8b4fb78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "data = pd.read_csv('car_evaluation.csv')\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "print(\"\\nLabel Encoded Data:\")\n",
    "print(data.head())\n",
    "\n",
    "data_one_hot = pd.get_dummies(data)\n",
    "\n",
    "print(\"\\nOne-Hot Encoded Data:\")\n",
    "print(data_one_hot.head())\n",
    "\n",
    "print(\"\\nNumber of features after Label Encoding:\", data.shape[1])\n",
    "print(\"Number of features after One-Hot Encoding:\", data_one_hot.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57dd6bb-576e-4831-8c2b-fae7c531bec8",
   "metadata": {},
   "source": [
    "3. Scaling Features in the Wine Quality Dataset\r\n",
    "   - Task: Apply normalization and standardization to the features in the Wine Quality dataset. Analyze how scaling affects the distribution of data.\r\n",
    "   - Dataset: Wine Quality Datast\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034cc65-91f2-40dc-a603-11d1f3e67d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "X = data.drop(columns='quality')\n",
    "y = data['quality']\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_normalized = min_max_scaler.fit_transform(X)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "X_standardized = standard_scaler.fit_transform(X)\n",
    "\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "X_standardized_df = pd.DataFrame(X_standardized, columns=X.columns)\n",
    "\n",
    "fig, axs = plt.subplots(3, len(X.columns), figsize=(20, 15))\n",
    "\n",
    "for i, column in enumerate(X.columns):\n",
    "    axs[0, i].hist(X[column], bins=20, color='blue', alpha=0.7)\n",
    "    axs[0, i].set_title(f'Original {column}')\n",
    "    \n",
    "    axs[1, i].hist(X_normalized_df[column], bins=20, color='green', alpha=0.7)\n",
    "    axs[1, i].set_title(f'Normalized {column}')\n",
    "    \n",
    "    axs[2, i].hist(X_standardized_df[column], bins=20, color='red', alpha=0.7)\n",
    "    axs[2, i].set_title(f'Standardized {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a2987-79a8-4038-a9b6-b940ea05cbda",
   "metadata": {},
   "source": [
    "\n",
    "4. Handling Outliers in the Boston Housing Dataset\r\n",
    "   - Task: Identify and handle outliers in the Boston Housing dataset using techniques like Z-score, IQR, and visualization methods.\r\n",
    "   - Dataset: Boston Housing Dataet\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d3cd6-78bc-4c0a-839e-2fcdf35c0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "data['MEDV'] = boston.target\n",
    "\n",
    "z_scores = np.abs(zscore(data))\n",
    "outliers_z = data[(z_scores > 3).any(axis=1)]\n",
    "\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = data[((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\\\n",
    "fig, axs = plt.subplots(2, len(data.columns)//2, figsize=(20, 10))\n",
    "fig.suptitle('Boxplots for each feature')\n",
    "\n",
    "for i, column in enumerate(data.columns):\n",
    "    sns.boxplot(y=data[column], ax=axs[i//7, i%7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data_no_outliers = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "print(f'Number of outliers detected by Z-score method: {outliers_z.shape[0]}')\n",
    "print(f'Number of outliers detected by IQR method: {outliers_iqr.shape[0]}')\n",
    "print(f'Number of data points after removing outliers using IQR method: {data_no_outliers.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c178d-9f5f-40b9-bc01-034d2f29c129",
   "metadata": {},
   "source": [
    "5. Data Imputation in the Retail Sales Dataset\n",
    "   - Task: Handle missing values in the Retail Sales dataset using advanced imputation techniques like KNN imputation and MICE.\n",
    "   - Dataset: Retail Sales Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22125a33-5071-4adc-af3d-31ecbbaaefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "data = pd.read_csv('retail_sales.csv')\n",
    "\n",
    "print(\"Initial dataset with missing values:\")\n",
    "print(data.head())\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "print(\"\\nDataset after KNN imputation:\")\n",
    "print(data_knn_imputed.head())\n",
    "\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "data_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "print(\"\\nDataset after MICE imputation:\")\n",
    "print(data_mice_imputed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae666ee-96b9-4d04-9d4f-69b01577ee7f",
   "metadata": {},
   "source": [
    "6. Feature Engineering in the Heart Disease Dataset\n",
    "   - Task: Create new features from existing ones in the Heart Disease dataset, such as age groups, cholesterol levels, and more.\n",
    "   - Dataset: Heart Disease Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545c6ce-0e06-412a-a931-9587ae4f97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "print(\"Initial dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "bins = [0, 29, 39, 49, 59, 69, 79, 89, 100]\n",
    "labels = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-89', '90-100']\n",
    "data['age_group'] = pd.cut(data['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "bins = [0, 200, 239, 500]\n",
    "labels = ['Normal', 'Borderline High', 'High']\n",
    "data['cholesterol_level'] = pd.cut(data['cholesterol'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "bins = [0, 120, 129, 139, 180, 300]\n",
    "labels = ['Normal', 'Elevated', 'High Blood Pressure (Hypertension Stage 1)', 'High Blood Pressure (Hypertension Stage 2)', 'Hypertensive Crisis']\n",
    "data['blood_pressure_category'] = pd.cut(data['trestbps'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "bins = [0, 100, 140, 160, 220]\n",
    "labels = ['Low', 'Below Average', 'Above Average', 'High']\n",
    "data['max_heart_rate_category'] = pd.cut(data['thalach'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(\"\\nDataset after feature engineering:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b4451-057c-44e3-97d2-9a1eba635ee0",
   "metadata": {},
   "source": [
    "7. Transforming Variables in the Bike Sharing Dataset\n",
    "   - Task: Apply transformations like log, square root, and Box-Cox transformations to skewed variables in the Bike Sharing dataset.\n",
    "   - Dataset: Bike Sharing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0185c-ada9-4a24-9ad0-c155e82e11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('bike_sharing.csv')\n",
    "\n",
    "print(\"Initial dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "skewed_features = ['count', 'temp', 'atemp', 'humidity', 'windspeed']\n",
    "\n",
    "data['count_log'] = np.log1p(data['count'])\n",
    "\n",
    "data['count_sqrt'] = np.sqrt(data['count'])\n",
    "\n",
    "data['count_boxcox'], fitted_lambda = boxcox(data['count'] + 1) \n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "\n",
    "for i, feature in enumerate(skewed_features):\n",
    "    sns.histplot(data[feature], bins=30, ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f'Distribution of {feature} (Original)')\n",
    "\n",
    "    if feature == 'count':\n",
    "        sns.histplot(data[f'{feature}_log'], bins=30, ax=axes[i, 1], kde=True)\n",
    "        axes[i, 1].set_title(f'Distribution of {feature} (Log Transformed)')\n",
    "    elif feature == 'count_sqrt':\n",
    "        sns.histplot(data[f'{feature}_sqrt'], bins=30, ax=axes[i, 1], kde=True)\n",
    "        axes[i, 1].set_title(f'Distribution of {feature} (Square Root Transformed)')\n",
    "    elif feature == 'count_boxcox':\n",
    "        sns.histplot(data[f'{feature}_boxcox'], bins=30, ax=axes[i, 1], kde=True)\n",
    "        axes[i, 1].set_title(f'Distribution of {feature} (Box-Cox Transformed)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataset after transformations:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad255bff-b3e0-4db2-9f61-9104f6fec3f6",
   "metadata": {},
   "source": [
    "8. Feature Selection in the Diabetes Dataset\n",
    "   - Task: Use techniques like correlation analysis, mutual information, and recursive feature elimination (RFE) to select important features in the Diabetes dataset.\n",
    "   - Dataset: Diabetes Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c38c7-b006-427a-845f-687324da4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "print(\"Initial dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "X = data.drop(columns='Outcome')\n",
    "y = data['Outcome']\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.2\n",
    "corr_features = correlation_matrix.index[abs(correlation_matrix['Outcome']) > threshold].tolist()\n",
    "print(f'Features with correlation greater than {threshold}: {corr_features}')\n",
    "\n",
    "mutual_info = mutual_info_classif(X, y)\n",
    "mutual_info_series = pd.Series(mutual_info, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "mutual_info_series.plot(kind='bar')\n",
    "plt.title('Mutual Information Scores')\n",
    "plt.show()\n",
    "print(f'Top features based on mutual information: {mutual_info_series.index.tolist()}')\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "fit = rfe.fit(X, y)\n",
    "rfe_features = X.columns[fit.support_].tolist()\n",
    "print(f'Selected features using RFE: {rfe_features}')\n",
    "\n",
    "selected_features = {\n",
    "    'Correlation': corr_features,\n",
    "    'Mutual Information': mutual_info_series.index.tolist(),\n",
    "    'RFE': rfe_features\n",
    "}\n",
    "print(\"\\nSelected features from each method:\")\n",
    "for method, features in selected_features.items():\n",
    "    print(f'{method}: {features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70096a-d072-421f-9a60-5f8988eb4be6",
   "metadata": {},
   "source": [
    "9. Dealing with Imbalanced Data in the Credit Card Fraud Detection Dataset\n",
    "   - Task: Handle imbalanced data in the Credit Card Fraud Detection dataset using techniques like SMOTE, ADASYN, and undersampling.\n",
    "   - Dataset: Credit Card Fraud Detection Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d1c18-066c-4ad1-9d3c-cb3abc07950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(\"Initial dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "X = data.drop(columns='Class')\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Original dataset evaluation:\")\n",
    "evaluate_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(\"SMOTE dataset evaluation:\")\n",
    "evaluate_model(X_train_smote, y_train_smote, X_test, y_test)\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "print(\"ADASYN dataset evaluation:\")\n",
    "evaluate_model(X_train_adasyn, y_train_adasyn, X_test, y_test)\n",
    "\n",
    "undersample = RandomUnderSampler(random_state=42)\n",
    "X_train_undersample, y_train_undersample = undersample.fit_resample(X_train, y_train)\n",
    "print(\"Undersampled dataset evaluation:\")\n",
    "evaluate_model(X_train_undersample, y_train_undersample, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33149078-4ae9-4131-a739-7412a066ec58",
   "metadata": {},
   "source": [
    "10. Combining Multiple Datasets in the Movie Lens Dataset\n",
    "    - Task: Combine and preprocess multiple related datasets from the Movie Lens dataset, such as ratings, user information, and movie metadata.\n",
    "    - Dataset: Movie Lens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd28071-55f1-44ee-91b2-b9f3666091ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "users = pd.read_csv('users.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "print(\"Ratings dataset:\")\n",
    "print(ratings.head())\n",
    "print(\"Users dataset:\")\n",
    "print(users.head())\n",
    "print(\"Movies dataset:\")\n",
    "print(movies.head())\n",
    "\n",
    "ratings_users = pd.merge(ratings, users, on='userId')\n",
    "complete_data = pd.merge(ratings_users, movies, on='movieId')\n",
    "\n",
    "print(\"Combined dataset:\")\n",
    "print(complete_data.head())\n",
    "\n",
    "complete_data.fillna(complete_data.mean(), inplace=True)\n",
    "\n",
    "complete_data['gender'] = complete_data['gender'].astype('category').cat.codes\n",
    "complete_data['occupation'] = complete_data['occupation'].astype('category').cat.codes\n",
    "\n",
    "complete_data['timestamp'] = pd.to_datetime(complete_data['timestamp'], unit='s')\n",
    "\n",
    "print(\"Preprocessed dataset:\")\n",
    "print(complete_data.head())\n",
    "\n",
    "complete_data.to_csv('combined_movie_lens_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
